{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words mode\n",
    "- each sentence should be a document\n",
    "- sub-documents for each file\n",
    "- heirachy:\n",
    "1. subject folder (corpus)\n",
    "2. sub-topics (corpus)\n",
    "3. files (corpus)\n",
    "4. documents (sentences (or 3) within the files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from urllib import request\n",
    "from urllib.request import FancyURLopener\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams , bigrams\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "# nltk.download('universal_tagset')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy\n",
    "from numpy import dot\n",
    "\n",
    "import sqlite3\n",
    "connection = sqlite3.connect('database.db')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn . model_selection import train_test_split\n",
    "from sklearn . feature_extraction . text import CountVectorizer\n",
    "from sklearn . feature_extraction . text import TfidfTransformer\n",
    "from sklearn . linear_model import LogisticRegression\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"medicine\"\n",
    "def find_topic(subject):\n",
    "    path = 'Datasets\\\\topic_list\\\\' + subject\n",
    "    for file in os.listdir(path):\n",
    "        with open(path, encoding='utf-8', errors='ignore', mode='r') as document:\n",
    "            content = document.read()\n",
    "            topic = document.name\n",
    "        print(topic)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def process_text(query):\n",
    "    #tokenization\n",
    "    tokenized_query = word_tokenize(query)\n",
    "\n",
    "    #removing stop words\n",
    "    #maybe make quotes a search prompt\n",
    "    tokens_wo_sw= [word for word in tokenized_query if not word in stopwords.words()]\n",
    "    filtered_query = (\"\").join(tokens_wo_sw)\n",
    "\n",
    "    postmap = {\n",
    "    'ADJ': 'j',\n",
    "    'ADV': 'r',\n",
    "    'NOUN': 'n',\n",
    "    'VERB': 'v'\n",
    "    }\n",
    "    \n",
    "    #lemmentised and tagged query\n",
    "    lemm_q = []\n",
    "    lemmentiser = WordNetLemmatizer()\n",
    "    tagged_query = nltk.pos_tag(lemm_q, tagset='universal')\n",
    "    for token in tagged_query: \n",
    "        word = token\n",
    "        tag = token[0]\n",
    "        tag = token[1]\n",
    "        if tag in postmap.keys():\n",
    "            lemm_q.append(lemmentiser.lemmatize(word, postmap[tag]))\n",
    "        else:\n",
    "            lemm_q.append(lemmentiser.lemmatize(word))\n",
    "\n",
    "    return lemm_q\n",
    "    # #only return false if no similar paper is found\n",
    "    # return falleback_response(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how long should each passage be?\n",
    "documents = {\n",
    "\"History\": \"Datasets\\\\battle_of_hastings.txt\",\n",
    "\"Medicine\": \"Datasets\\\\biasesinnlp.txt\",\n",
    "\"ComputerScience\": \"Datasets\\\\brain-injury.txt\"\n",
    "}\n",
    "\n",
    "topic = \"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = {}\n",
    "for subject in documents:\n",
    "    with open(documents.get(subject), encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "        corpus[subject] = f.read()\n",
    "\n",
    "\n",
    "all_text = corpus.values()\n",
    "\n",
    "#binary = 0 because we arent interested in text where the keywords are not there\n",
    "count_vect = CountVectorizer(stop_words=stopwords.words('english'), binary=0, ngram_range=(1,3))\n",
    "X_train_counts = count_vect.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "[',', '.', 'fine', 'mangle', 'process', 'sentence', 'stem', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn . feature_extraction . text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer( stop_words = stopwords.words('english'))\n",
    "X_train_counts = count_vect.fit_transform ( all_text )\n",
    "tf_transformer = TfidfTransformer ( use_idf = True , sublinear_tf = True ). fit (X_train_counts )\n",
    "X_train_tf = tf_transformer.transform( X_train_counts )\n",
    "# print ( X_train_tf.shape )\n",
    "\n",
    "\n",
    "\n",
    "lemmentiser = WordNetLemmatizer()\n",
    "analyser = CountVectorizer().build_analyzer()\n",
    "\n",
    "def lemmed_words (text):\n",
    "    tokenized_query = word_tokenize(text)\n",
    "    tokens_wo_sw= [word.lower() for word in tokenized_query if not word in stopwords.words()]\n",
    "    # print(tokens_wo_sw)\n",
    "    \n",
    "    postmap = {\n",
    "    'ADJ': 'j',\n",
    "    'ADV': 'r',\n",
    "    'NOUN': 'n',\n",
    "    'VERB': 'v'\n",
    "    }\n",
    "    \n",
    "    #lemmentised and tagged query\n",
    "    lemm_q = []\n",
    "    lemmentiser = WordNetLemmatizer()\n",
    "    tagged_query = nltk.pos_tag(tokens_wo_sw, tagset='universal')\n",
    "    for token in tagged_query: \n",
    "        word = token[0]\n",
    "        tag = token[1]\n",
    "        if tag in postmap.keys():\n",
    "            lemm_q.append(lemmentiser.lemmatize(word, postmap[tag]))\n",
    "        else:\n",
    "            lemm_q.append(lemmentiser.lemmatize(word))\n",
    "    \n",
    "    return lemm_q\n",
    "\n",
    "lemm_vectorizer = CountVectorizer( analyzer = lemmed_words )\n",
    "print ( lemm_vectorizer.fit_transform(['This sentence should get seriously mangled in the stemming process, but it is fine.']) )\n",
    "print(lemm_vectorizer.get_feature_names ())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the chatbot\n",
    "- classifying parts of text\n",
    "- maybe should be able to add new classess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_dir = {\n",
    "\"history\": \"Datasets/training/history\",\n",
    "\"medicine\": \"Datasets/training/medicine\",\n",
    "\"greetings\": \"Datasets/training/greetings\"\n",
    "}\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for label in label_dir . keys () :\n",
    "    for file in os . listdir ( label_dir [ label ]) :\n",
    "        filepath = label_dir [ label ] + os . sep + file\n",
    "        with open(filepath , encoding ='utf8', errors ='ignore', mode ='r') as review:\n",
    "            content = review . read ()\n",
    "            data . append ( content )\n",
    "            labels . append ( label )\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split ( data , labels , stratify = labels, test_size =0.25 , random_state =1)\n",
    "\n",
    "count_vect = CountVectorizer (stop_words = stopwords . words ('english'))\n",
    "X_train_counts = count_vect . fit_transform ( X_train )\n",
    "\n",
    "tfidf_transformer = TfidfTransformer ( use_idf = True , sublinear_tf = True ). fit (\n",
    "X_train_counts )\n",
    "X_train_tf = tfidf_transformer . transform ( X_train_counts )\n",
    "\n",
    "classifier = LogisticRegression ( random_state =0) . fit ( X_train_tf , y_train )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how many bones are in the human body?', 'hello and good morning!', 'who was the first emperor of ancient rome?', 'when was the battle of hastings?', 'hi! hello! how are you?', 'greetings! how are you today?', 'what are the effects of smoking on the lungs?'] ['medicine', 'greetings', 'history', 'history', 'greetings', 'greetings', 'medicine']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score , f1_score , confusion_matrix\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "filepath = \"Datasets/training/\"\n",
    "for label in label_dir . keys () :\n",
    "    for file in os . listdir ( label_dir [ label ]) :\n",
    "        filepath = label_dir [ label ] + os . sep + file\n",
    "        with open(filepath , encoding ='utf8', errors ='ignore', mode ='r') as review:\n",
    "            content = review.read()\n",
    "            data.append (content )\n",
    "            labels.append ( label )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify=labels, test_size=0.25, random_state=42)\n",
    "print(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 19)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 24)\t1\n",
      "  (3, 26)\t1\n",
      "  (3, 20)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 11)\t1\n",
      "  (4, 14)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 12)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 30)\t1\n",
      "  (5, 14)\t1\n",
      "  (5, 2)\t1\n",
      "  (5, 30)\t1\n",
      "  (5, 10)\t1\n",
      "  (5, 25)\t1\n",
      "  (6, 2)\t1\n",
      "  (6, 24)\t2\n",
      "  (6, 20)\t1\n",
      "  (6, 27)\t1\n",
      "  (6, 6)\t1\n",
      "  (6, 23)\t1\n",
      "  (6, 21)\t1\n",
      "  (6, 17)\t1\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (p_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, stop_words=stopwords.words('english'), analyzer=stemmed_words)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 24)\t0.2461638358943208\n",
      "  (0, 18)\t0.3996040294183947\n",
      "  (0, 16)\t0.3996040294183947\n",
      "  (0, 15)\t0.3996040294183947\n",
      "  (0, 14)\t0.28353100387524055\n",
      "  (0, 5)\t0.3996040294183947\n",
      "  (0, 4)\t0.3996040294183947\n",
      "  (0, 2)\t0.2461638358943208\n",
      "  (1, 19)\t0.5206467559864713\n",
      "  (1, 12)\t0.43218152024617124\n",
      "  (1, 9)\t0.5206467559864713\n",
      "  (1, 1)\t0.5206467559864713\n",
      "  (2, 29)\t0.3900791205824988\n",
      "  (2, 26)\t0.323799169804132\n",
      "  (2, 24)\t0.24029630723351017\n",
      "  (2, 22)\t0.3900791205824988\n",
      "  (2, 20)\t0.27677279633666213\n",
      "  (2, 8)\t0.3900791205824988\n",
      "  (2, 7)\t0.3900791205824988\n",
      "  (2, 0)\t0.3900791205824988\n",
      "  (3, 28)\t0.4676802387867398\n",
      "  (3, 26)\t0.3882147622431314\n",
      "  (3, 24)\t0.2881001017914568\n",
      "  (3, 20)\t0.3318331093628171\n",
      "  (3, 11)\t0.4676802387867398\n",
      "  (3, 3)\t0.4676802387867398\n",
      "  (4, 30)\t0.459671713122356\n",
      "  (4, 14)\t0.39291214216113446\n",
      "  (4, 13)\t0.553764043681409\n",
      "  (4, 12)\t0.459671713122356\n",
      "  (4, 2)\t0.34112939594570285\n",
      "  (5, 30)\t0.43920778363284724\n",
      "  (5, 25)\t0.5291112577469715\n",
      "  (5, 14)\t0.3754202536171529\n",
      "  (5, 10)\t0.5291112577469715\n",
      "  (5, 2)\t0.3259428014563147\n",
      "  (6, 27)\t0.3787556651431189\n",
      "  (6, 24)\t0.3950465206621853\n",
      "  (6, 23)\t0.3787556651431189\n",
      "  (6, 21)\t0.3787556651431189\n",
      "  (6, 20)\t0.2687384662205801\n",
      "  (6, 17)\t0.3787556651431189\n",
      "  (6, 6)\t0.3787556651431189\n",
      "  (6, 2)\t0.23332083896660324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, sublinear_tf=True).fit(X_train_counts)\n",
    "X_train_tf = tfidf_transformer.transform(X_train_counts)\n",
    "\n",
    "print(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n",
      "0.6666666666666666\n",
      "0.5555555555555555\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing documents and creating term-document matrix\n",
    "X_new_counts = count_vect.transform(X_test)\n",
    "\n",
    "# Weighting\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Predict on the test set\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "# Print metrics\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(accuracy_score(y_test, predicted))\n",
    "print(f1_score(y_test, predicted, pos_label='greetings', average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527c535e79d9b82e952da264c75bcf6572fc08e659129c89b911d6618328dece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
